{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745825277.063244 3713459 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745825277.063624 3713459 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745825277.067549 3713757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745825277.075643 3713757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745825277.138228 3713459 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745825277.138483 3713459 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745825277.142136 3713773 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745825277.150147 3713773 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745825277.153408 3713459 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745825277.153664 3713459 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745825277.157616 3713783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745825277.165726 3713783 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Head Pose - Pitch: -0.19, Yaw: -0.25, Roll: 1.09\n",
      "all_data_output(head_pose_angles=(-0.192516749688779, -0.25367589477705477, 1.0939493273312475), face_box=((774, 519), (1083, 902)), left_eye_box=((950, 639), (1022, 669)), right_eye_box=((836, 640), (904, 666)), eye_iris_center=((983, 653), (869, 652)), eye_iris_left_box=((970, 641), (996, 665)), eye_iris_right_box=((857, 641), (882, 665)), eye_centers=((986, 656), (869, 655), (927, 655)), landmark_positions={'Nose': (929, 716), 'Mouth_L': (885, 791), 'Mouth_R': (982, 787), 'Chin': (936, 877), 'Cheek_L': (801, 703), 'Cheek_R': (1058, 698)}, left_eye_state=('Open', np.float64(0.2776273191632896)), right_eye_state=('Open', np.float64(0.27117300405376954)), depths=(np.float64(70.32), np.float64(71.12), np.float64(72.05), np.float64(71.37)))\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import the main class from the provided code\n",
    "# Assuming the provided code is saved as face_tracker.py\n",
    "from Main_model.showframeVisualization import FrameShow_head_face\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Initialize the face tracker\n",
    "    face_tracker = FrameShow_head_face(\n",
    "        model_path='Main_model/face_landmarker.task',\n",
    "        isVideo=False,  # Set to False for static image processing\n",
    "        isHeadposeOn=False,\n",
    "        isFaceOn=True\n",
    "    )\n",
    "    \n",
    "    # # Enable visualization features\n",
    "    face_tracker.set_labet_face_element(False)\n",
    "    # face_tracker.set_IsShowBox(True)\n",
    "    # face_tracker.set_IsShowHeadpose(True)\n",
    "    \n",
    "    # Create window\n",
    "    window_name = \"Face Tracking Demo\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    # Image path (fixed from your example)\n",
    "    # image_path = '/Users/porchportal2/Desktop/Screenshot 2568-03-17 at 12.33.54.png'\n",
    "    image_path = '/Users/porchportal2/Desktop/ðŸ”¥everything/eyeTrackingProject/Program/ðŸ“ŒFace_Head/special_dataset/webcam_136.jpg'\n",
    "    \n",
    "    # Load the image properly\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        print(f\"Error: Could not read image from {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Process the frame (timestamp_ms is only relevant for video mode)\n",
    "    timestamp_ms = 0\n",
    "    metrics, processed_frame = face_tracker.process_frame(frame, timestamp_ms, isVideo=False, isEnhanceFace=False)\n",
    "    \n",
    "    if metrics is None:\n",
    "        print(\"No face detected in the image.\")\n",
    "    else:\n",
    "        # Print face analysis results\n",
    "        pitch, yaw, roll = metrics.head_pose_angles\n",
    "        print(f\"Head Pose - Pitch: {pitch:.2f}, Yaw: {yaw:.2f}, Roll: {roll:.2f}\")\n",
    "        print(metrics)\n",
    "    \n",
    "    # Display the processed image\n",
    "    cv2.imshow(window_name, processed_frame)\n",
    "    cv2.waitKey(0)  # Wait for a key press\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb87dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:33:06.782 python[14221:3831013] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745911988.206105 3831013 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745911988.214675 3831013 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745911988.225768 4446964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745911988.238656 4446964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745911988.322532 3831013 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745911988.322803 3831013 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745911988.327898 4446997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745911988.336727 4447000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1745911988.339731 3831013 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Max\n",
      "W0000 00:00:1745911988.339979 3831013 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1745911988.344452 4447007 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745911988.352747 4447009 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Face tracking started. Press 'q' to quit.\n",
      "Controls:\n",
      "  s: Toggle face element labels\n",
      "  d: Toggle bounding box\n",
      "  f: Toggle head pose visualization\n",
      "  g: Toggle face mask\n",
      "  e: Toggle value output\n",
      "  z: Toggle face zoom\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "ValueError: not enough values to unpack (expected 5, got 2)\n",
      "--------------------------------------------------\n",
      "Eye Left: (970, 446), Right: (801, 446)\n",
      "Box Eye Left: Min((920, 427)), Max((1022, 462))\n",
      "Box Eye Right: Min((755, 431)), Max((847, 460))\n",
      "Head-pose: Pitch-0.22, Yaw0.07, Roll-0.00\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 442), Right: (801, 442)\n",
      "Box Eye Left: Min((918, 423)), Max((1021, 459))\n",
      "Box Eye Right: Min((755, 425)), Max((847, 457))\n",
      "Head-pose: Pitch-0.22, Yaw0.07, Roll0.00\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 440), Right: (801, 439)\n",
      "Box Eye Left: Min((919, 420)), Max((1020, 457))\n",
      "Box Eye Right: Min((755, 422)), Max((847, 454))\n",
      "Head-pose: Pitch-0.19, Yaw0.08, Roll-0.01\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 439), Right: (801, 438)\n",
      "Box Eye Left: Min((919, 420)), Max((1020, 457))\n",
      "Box Eye Right: Min((756, 421)), Max((847, 453))\n",
      "Head-pose: Pitch-0.17, Yaw0.09, Roll-0.01\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 439), Right: (801, 438)\n",
      "Box Eye Left: Min((919, 420)), Max((1020, 456))\n",
      "Box Eye Right: Min((756, 421)), Max((847, 453))\n",
      "Head-pose: Pitch-0.18, Yaw0.08, Roll-0.01\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 438), Right: (801, 437)\n",
      "Box Eye Left: Min((920, 419)), Max((1020, 455))\n",
      "Box Eye Right: Min((756, 420)), Max((847, 452))\n",
      "Head-pose: Pitch-0.21, Yaw0.07, Roll-0.00\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 438), Right: (802, 436)\n",
      "Box Eye Left: Min((920, 418)), Max((1020, 455))\n",
      "Box Eye Right: Min((756, 419)), Max((848, 451))\n",
      "Head-pose: Pitch-0.21, Yaw0.06, Roll0.00\n",
      "--------------------------------------------------\n",
      "Eye Left: (969, 437), Right: (802, 436)\n",
      "Box Eye Left: Min((920, 418)), Max((1020, 454))\n",
      "Box Eye Right: Min((756, 419)), Max((849, 451))\n",
      "Head-pose: Pitch-0.21, Yaw0.06, Roll0.00\n",
      "--------------------------------------------------\n",
      "Eye Left: (906, 429), Right: (717, 432)\n",
      "Box Eye Left: Min((850, 408)), Max((965, 449))\n",
      "Box Eye Right: Min((667, 415)), Max((768, 448))\n",
      "Head-pose: Pitch-0.21, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (906, 429), Right: (717, 432)\n",
      "Box Eye Left: Min((850, 408)), Max((965, 449))\n",
      "Box Eye Right: Min((667, 415)), Max((768, 448))\n",
      "Head-pose: Pitch-0.21, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (906, 428), Right: (717, 432)\n",
      "Box Eye Left: Min((850, 407)), Max((965, 448))\n",
      "Box Eye Right: Min((667, 414)), Max((768, 448))\n",
      "Head-pose: Pitch-0.21, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (906, 428), Right: (717, 431)\n",
      "Box Eye Left: Min((850, 407)), Max((965, 448))\n",
      "Box Eye Right: Min((667, 414)), Max((768, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (907, 427), Right: (718, 431)\n",
      "Box Eye Left: Min((850, 406)), Max((966, 448))\n",
      "Box Eye Right: Min((667, 413)), Max((769, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (907, 427), Right: (719, 431)\n",
      "Box Eye Left: Min((851, 406)), Max((966, 447))\n",
      "Box Eye Right: Min((668, 413)), Max((770, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (909, 427), Right: (720, 431)\n",
      "Box Eye Left: Min((852, 406)), Max((967, 447))\n",
      "Box Eye Right: Min((670, 413)), Max((771, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.10, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (911, 428), Right: (723, 431)\n",
      "Box Eye Left: Min((855, 407)), Max((969, 448))\n",
      "Box Eye Right: Min((673, 414)), Max((774, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.09, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (913, 428), Right: (724, 431)\n",
      "Box Eye Left: Min((856, 407)), Max((971, 448))\n",
      "Box Eye Right: Min((674, 414)), Max((776, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.09, Roll0.03\n",
      "--------------------------------------------------\n",
      "Eye Left: (914, 428), Right: (725, 431)\n",
      "Box Eye Left: Min((858, 407)), Max((972, 448))\n",
      "Box Eye Right: Min((675, 414)), Max((777, 448))\n",
      "Head-pose: Pitch-0.20, Yaw0.09, Roll0.03\n",
      "\n",
      "Program terminated by user\n",
      "Resources released. Program ended.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings and low-level logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # Disable GPU warnings if not using GPU\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'  # Suppress Python warnings\n",
    "\n",
    "# Configure logging to suppress non-critical warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Import other libraries (after setting environment variables)\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize absl logging before importing MediaPipe-related modules\n",
    "try:\n",
    "    from absl import logging as absl_logging\n",
    "    absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "    # Additional absl flags configuration to minimize warnings\n",
    "    from absl import flags\n",
    "    flags.FLAGS.mark_as_parsed()\n",
    "except ImportError:\n",
    "    print(\"Warning: absl module not found, some warnings may still appear\")\n",
    "\n",
    "# Now import your custom module\n",
    "from Main_model.showframeVisualization import FrameShow_head_face\n",
    "\n",
    "class Toggle:\n",
    "    def __init__(self, initial_state=False):\n",
    "        self.state = initial_state\n",
    "    \n",
    "    def toggle(self):\n",
    "        self.state = not self.state\n",
    "        return self.state\n",
    "    \n",
    "    def __bool__(self):\n",
    "        return self.state\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(1)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open camera. Trying default camera (0)...\")\n",
    "            cap = cv2.VideoCapture(0)\n",
    "            if not cap.isOpened():\n",
    "                print(\"Error: No camera available. Exiting.\")\n",
    "                return\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        min_width = 1280\n",
    "        min_height = 720\n",
    "\n",
    "        # Get the maximum resolution supported by the camera\n",
    "        max_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        max_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, max(max_width, min_width))\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, max(max_height, min_height))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        isShow_direction = Toggle(False)\n",
    "        isShow_label = Toggle(False)\n",
    "        isShow_box = Toggle(False)\n",
    "        isShow_mask = Toggle(False)\n",
    "        IsValueOutput = Toggle(False)\n",
    "        isZoomFace = Toggle(False)\n",
    "        \n",
    "        # Create tracker with error handling\n",
    "        try:\n",
    "            tracker = FrameShow_head_face(\n",
    "                model_path='Main_model/face_landmarker.task', \n",
    "                isHeadposeOn=True, \n",
    "                isVideo=True,\n",
    "                isFaceOn=True \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing face tracker: {e}\")\n",
    "            print(\"Make sure 'face_landmarker.task' file exists in the current directory\")\n",
    "            return\n",
    "        \n",
    "        print(\"Face tracking started. Press 'q' to quit.\")\n",
    "        print(\"Controls:\")\n",
    "        print(\"  s: Toggle face element labels\")\n",
    "        print(\"  d: Toggle bounding box\")\n",
    "        print(\"  f: Toggle head pose visualization\")\n",
    "        print(\"  g: Toggle face mask\")\n",
    "        print(\"  e: Toggle value output\")\n",
    "        print(\"  z: Toggle face zoom\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Failed to grab frame from camera\")\n",
    "                print(\"Try reconnecting your camera or checking permissions\")\n",
    "                break\n",
    "            \n",
    "            h, w = frame.shape[:2]\n",
    "            x_axis = int(w // 1.5)\n",
    "            current_time = time.time()\n",
    "            timestamp_ms = int((current_time - start_time) * 1000)\n",
    "            \n",
    "            try:\n",
    "                # Process frame with current timestamp\n",
    "                matrix_out, frame = tracker.process_frame(frame, timestamp_ms)\n",
    "                \n",
    "                # Check if face was detected\n",
    "                face_detected = matrix_out is not None\n",
    "                status_color = (0, 255, 0) if face_detected else (0, 0, 255)\n",
    "                \n",
    "                # Add face detection status\n",
    "                cv2.putText(frame, f\"Face detected: {face_detected}\", \n",
    "                           (x_axis, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)\n",
    "                \n",
    "                # Add controls info\n",
    "                cv2.putText(frame, f\"(s) show all label parameters: {bool(isShow_label)}\", \n",
    "                           (x_axis, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"(d) show bounding box face: {bool(isShow_box)}\", \n",
    "                           (x_axis, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"(f) show visualization headpose: {bool(isShow_direction)}\", \n",
    "                           (x_axis, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"(g) show mask on face: {bool(isShow_mask)}\", \n",
    "                           (x_axis, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"(e) Value Output: {bool(IsValueOutput)}\", \n",
    "                           (x_axis, 280), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"(z) Zoom: {bool(isZoomFace)}\", \n",
    "                           (x_axis, 320), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "                \n",
    "                # Make sure frame is properly formatted\n",
    "                if isinstance(frame, list):  # If it's a list, convert to NumPy array\n",
    "                    frame = np.array(frame, dtype=np.uint8)\n",
    "                \n",
    "                # Display the frame\n",
    "                if frame is not None and frame.size > 0:\n",
    "                    cv2.imshow('Head Pose Estimation', frame)\n",
    "                else:\n",
    "                    print(\"Error: Invalid frame!\")\n",
    "                \n",
    "                # Only output values if face is detected AND value output is enabled\n",
    "                if bool(IsValueOutput) and face_detected:\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Eye Left: {matrix_out.eye_centers[0]}, Right: {matrix_out.eye_centers[1]}\")\n",
    "                    print(f\"Box Eye Left: Min({matrix_out.left_eye_box[0]}), Max({matrix_out.left_eye_box[1]})\")\n",
    "                    print(f\"Box Eye Right: Min({matrix_out.right_eye_box[0]}), Max({matrix_out.right_eye_box[1]})\")\n",
    "                    print(f\"Head-pose: Pitch{matrix_out.head_pose_angles[0]:.2f}, Yaw{matrix_out.head_pose_angles[1]:.2f}, Roll{matrix_out.head_pose_angles[2]:.2f}\")\n",
    "                elif bool(IsValueOutput) and not face_detected:\n",
    "                    print(\"No face detected - cannot display metrics\")\n",
    "                    \n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    isShow_label.toggle()\n",
    "                    tracker.set_labet_face_element(bool(isShow_label))\n",
    "                elif key == ord('d'):\n",
    "                    isShow_box.toggle()\n",
    "                    tracker.set_IsShowBox(bool(isShow_box))\n",
    "                elif key == ord('f'):\n",
    "                    isShow_direction.toggle()\n",
    "                    tracker.set_IsShowHeadpose(bool(isShow_direction))\n",
    "                elif key == ord('g'):\n",
    "                    isShow_mask.toggle()\n",
    "                    tracker.set_IsMaskOn(bool(isShow_mask))\n",
    "                elif key == ord('e'):\n",
    "                    IsValueOutput.toggle()\n",
    "                elif key == ord('z'):\n",
    "                    isZoomFace.toggle()\n",
    "                    tracker.set_IsZoomFace(bool(isZoomFace))\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                if \"Input timestamp must be monotonically increasing\" in str(e):\n",
    "                    # Reset timestamp if we get an error\n",
    "                    print(\"Timestamp error - resetting time tracking\")\n",
    "                    start_time = time.time()\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"ValueError: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {str(e)}\")\n",
    "                # Continue running despite errors\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProgram terminated by user\")\n",
    "    finally:\n",
    "        if 'cap' in locals() and cap.isOpened():\n",
    "            cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Resources released. Program ended.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eyeTracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
